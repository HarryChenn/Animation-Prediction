#Import Required Packages
We first import the packages we need such as gensim, pandas, nltk tokenizers, glob....

import gensim 

from nltk.tokenize import sent_tokenize
from nltk.tokenize.treebank import TreebankWordTokenizer
import nltk
nltk.download('punkt')
import glob
from pathlib import Path
from bs4 import BeautifulSoup 
import re 
from sklearn import feature_extraction
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib Inline
import seaborn as sns
import os

# Load anime data
You need to use your Kaggle public API to get the access to the dataset

! pip install -q kaggle

from google.colab import files

files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! mkdir reviews

!kaggle datasets download -d marlesson/myanimelist-dataset-animes-profiles-reviews -p /reviews --unzip

df_anime = pd.read_csv('/reviews/animes.csv')

# Now we are spliting the animations into three groups

# Anime's synopsis from 0 to 5000
pop1 = []
# Anime's synopsis from 5000 to 10000
pop2 = []
# Anime's synopsis from 10000 to 150000
pop3 = []

for i in df_anime['popularity']:
  if(i < 5000):
    pop1.append(df_anime.synopsis[i])
  if(5000 < i < 10000):
    pop2.append(df_anime.synopsis[i])
  if(10000 < i):
    pop3.append(df_anime.synopsis[i])
    
# Corpus Preprocessing
import logging 
import itertools
import gensim
# configure logging, since topic modeling takes a while and it's good to know what's going on 
logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)
logging.root.level = logging.INFO  
def head(stream, n=10):
    return list(itertools.islice(stream, n))

from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS

def tokenize(text):
    return [token for token in simple_preprocess(text) if token not in STOPWORDS]
# list for tokenized documents in loop
pop1texts = []

for i in pop1:
    # clean and tokenize document string
    raw = str(i).lower()
    tokens = tokenize(raw)
    # add tokens to list
    pop1texts.append(tokens)
 pop2texts = []

for i in pop2:
  raw = str(i).lower()
  tokens = tokenize(raw)
  pop2texts.append(tokens)
pop3texts = []

for i in pop3:
  raw = str(i).lower()
  tokens = tokenize(raw)
  pop3texts.append(tokens)

# Run the Method For Popularity Group One

def iter_docs(synopsis_list):
  for id, synopsis in enumerate(synopsis_list):
    yield id, tokenize(synopsis)

pop1id2word_synopsis = gensim.corpora.Dictionary(pop1texts) 
print(pop1id2word_synopsis)
pop1id2word_synopsis.token2id
pop1id2word_synopsis.filter_extremes(no_below=2, no_above=1.0)
pop1id2word_synopsis
class Corpus(object):
    def __init__(self, dump_file, dictionary, clip_docs=None):
        self.dump_file = dump_file
        self.dictionary = dictionary
        self.clip_docs = clip_docs
    
    def __iter__(self):
        self.titles = []
        for title, tokens in itertools.islice(iter_docs(self.dump_file), self.clip_docs):
            self.titles.append(title)
            yield self.dictionary.doc2bow(tokens)
    
    def __len__(self):
        return self.clip_docs
        
pop1synopsis = ''.join(pop1texts[0])
pop1anime_corpus = Corpus(pop1synopsis, pop1id2word_synopsis)
from gensim import corpora, models
pop1dictionary = corpora.Dictionary(pop1texts)
# convert tokenized documents into a document-term matrix
pop1corpus = [pop1dictionary.doc2bow(text) for text in pop1texts]
pop1ldamodel = gensim.models.ldamodel.LdaModel(pop1corpus, num_topics=20, id2word = pop1dictionary, passes=50)
import pprint
pprint.pprint(pop1ldamodel.top_topics(pop1corpus,topn=5))

pop1topics = pop1ldamodel.show_topics(20, 20, formatted=False)

for topic in pop1topics:
    topic_num = topic[0]
    topic_words = ""
    
    topic_pairs = topic[1]
    for pair in topic_pairs:
        topic_words += pair[0] + ", "
    
    print("T" + str(topic_num) + ": " + topic_words)

pop2id2word_synopsis = gensim.corpora.Dictionary(pop2texts) 
print(pop2id2word_synopsis)
pop2id2word_synopsis.token2idpop2id2word_synopsis.filter_extremes(no_below=2, no_above=1.0)

pop2id2word_synopsis
pop2synopsis = ''.join(pop2texts[0])
pop2anime_corpus = Corpus(pop2synopsis, pop2id2word_synopsis)
from gensim import corpora, models
pop2dictionary = corpora.Dictionary(pop2texts)
# convert tokenized documents into a document-term matrix
pop2corpus = [pop2dictionary.doc2bow(text) for text in pop2texts]
pop2ldamodel = gensim.models.ldamodel.LdaModel(pop2corpus, num_topics=20, id2word = pop2dictionary, passes=50)
import pprint
pprint.pprint(pop2ldamodel.top_topics(pop2corpus,topn=5))
pop2topics = pop2ldamodel.show_topics(20, 20, formatted=False)

for topic in pop2topics:
    topic_num = topic[0]
    topic_words = ""
    
    topic_pairs = topic[1]
    for pair in topic_pairs:
        topic_words += pair[0] + ", "
    
    print("T" + str(topic_num) + ": " + topic_words)
